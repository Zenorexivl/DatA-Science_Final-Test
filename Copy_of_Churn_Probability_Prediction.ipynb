{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 18858,
          "sourceType": "datasetVersion",
          "datasetId": 13996
        }
      ],
      "dockerImageVersionId": 30684,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zenorexivl/Task/blob/main/Copy_of_Churn_Probability_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'telco-customer-churn:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F13996%2F18858%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240609%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240609T043621Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6335c2010b498c0155bbbb458cb1c3eb69970fa0bf84e26ce7559c5acc0e7fc6502d68895f9e59f43ae0df1022bd10d31d0565236a01e9e1a1b1603d1a8163d0c3cd721509c0266fcafcfff10356e058a1810be9bb41da93f57dee798b74b3e052a818973b8531729f9a3aed51cc5715ac938e66ecc65b08b0a222b914c157e87d8f5057f16a30424c06dd32a7757fd0620ba1cd1809e0692b4d0d7263143dbcd9b2c5f11448c8380e1664a1ff6932fccb854d5fbd520cfb4d52408b6aecfa7dee2899a115f06fbdf77456407dd268b56c35018799a412c93d9df40eff90f377328e51ccb6b0276774b319abc67c64611f25f1b1706c2a090e2f435f17c18dc2'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "FABpap4SSI5r"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal** - In this notebook, I will demonstrate the Machine Learning Project workflow for Classification. I have picked a customer churn dataset available on IBM sample Data Sets for this exercise. (https://community.ibm.com/community/user/businessanalytics/blogs/steven-macko/2019/07/11/telco-customer-churn-1113)\n",
        "\n",
        "**Business Problem** -\n",
        "A fictional telco company provides home phone and Internet services to 7043 customers in California. We have customer churn data indicating which customers left, stayed or signedup for their services in the last month. Demographics and etc data is also vailable for each customer.\n",
        "\n",
        "We want to explore this data set to understand what impacts customer churn and develop a predictive model to identify customer's liklihood to churn.\n",
        "![image.png](attachment:1388cd06-8ac1-412f-984a-1437fcc48665.png)\n"
      ],
      "metadata": {
        "id": "KeE9KSshSI5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Python Libraries"
      ],
      "metadata": {
        "id": "2C8e6ydbSI5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import python libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.metrics as m\n",
        "from sklearn.metrics import  roc_curve , auc , confusion_matrix, log_loss, brier_score_loss, f1_score\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "\n",
        "import sklearn.model_selection as cv\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import  roc_curve , auc , confusion_matrix\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc, precision_score, recall_score\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "import warnings\n",
        "\n",
        "# Mute all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:44.743322Z",
          "iopub.execute_input": "2024-06-04T19:41:44.743708Z",
          "iopub.status.idle": "2024-06-04T19:41:48.096984Z",
          "shell.execute_reply.started": "2024-06-04T19:41:44.743678Z",
          "shell.execute_reply": "2024-06-04T19:41:48.095842Z"
        },
        "trusted": true,
        "id": "WkRYxUVrSI5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingest Data"
      ],
      "metadata": {
        "id": "fgJDTM49SI5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read data in pandas dataframe\n",
        "df = pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "#get a sense of the data\n",
        "df.head()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:48.099028Z",
          "iopub.execute_input": "2024-06-04T19:41:48.099504Z",
          "iopub.status.idle": "2024-06-04T19:41:48.207332Z",
          "shell.execute_reply.started": "2024-06-04T19:41:48.09947Z",
          "shell.execute_reply": "2024-06-04T19:41:48.206314Z"
        },
        "trusted": true,
        "id": "0FCxc4_zSI5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#identify the number of records and features/columns in this data set\n",
        "print(f'The number of records are {df.shape[0]}')\n",
        "print(f'The number of features inluding the churn predictor variable are {df.shape[1]}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:48.208862Z",
          "iopub.execute_input": "2024-06-04T19:41:48.209634Z",
          "iopub.status.idle": "2024-06-04T19:41:48.21583Z",
          "shell.execute_reply.started": "2024-06-04T19:41:48.209594Z",
          "shell.execute_reply": "2024-06-04T19:41:48.214657Z"
        },
        "trusted": true,
        "id": "I9pOe4rnSI5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "beckZWxXSI5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check for Null values. Calculate the Churn Rate**"
      ],
      "metadata": {
        "id": "WaxuuK1RSI5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#explore the dependent variable 'Churn'\n",
        "\n",
        "#check if there are any null values for 'Churn'\n",
        "print(f\"number of records with null values for Churn are {df['Churn'].isnull().sum()}\")\n",
        "\n",
        "# we want to see the distribution of customers for distinct Churn values\n",
        "\n",
        "print(df.groupby('Churn')['customerID'].count())\n",
        "\n",
        "#Note - here Yes means customer has churned/ left the company while No means the customer stayed\n",
        "\n",
        "#Convert Chrun to binary variable such that Yes = 1 and No = 0. This is the target variable\n",
        "# we want to predict liklihood of churn hence we will map Yes to 1 for future classification model\n",
        "\n",
        "df['Churn_Target'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "#calculate churn rate i.e. number of customer churned/ total customers\n",
        "\n",
        "print(f\"Churn rate is {sum(df['Churn_Target'])*100/ len(df['Churn_Target'])}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:48.217163Z",
          "iopub.execute_input": "2024-06-04T19:41:48.217538Z",
          "iopub.status.idle": "2024-06-04T19:41:48.243587Z",
          "shell.execute_reply.started": "2024-06-04T19:41:48.217505Z",
          "shell.execute_reply": "2024-06-04T19:41:48.242478Z"
        },
        "trusted": true,
        "id": "grov8Vn4SI5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if there are null values in the data\n",
        "\n",
        "null_vals = df.isnull().sum()\n",
        "\n",
        "print(f'columns with null values: {null_vals[null_vals > 0]}')\n",
        "#No null values\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:48.247813Z",
          "iopub.execute_input": "2024-06-04T19:41:48.248244Z",
          "iopub.status.idle": "2024-06-04T19:41:48.270956Z",
          "shell.execute_reply.started": "2024-06-04T19:41:48.248201Z",
          "shell.execute_reply": "2024-06-04T19:41:48.269752Z"
        },
        "trusted": true,
        "id": "lfgtMm_LSI5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**check descriptive statistics of the data **"
      ],
      "metadata": {
        "id": "k0hIT2YRSI5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.describe()\n",
        "#OBSERVATIONS\n",
        "# - We had total 21 features but we are seeing descriptive stats of only few. Other features are categorical.\n",
        "# - The SeniorCitien feature is binary\n",
        "# - In Cell 6, we saw 5 rows from the data which included TotalCharges feature.\n",
        "#         It is numeric variable but we cannot see it in  describe output below\n",
        "# - Let us investigate it"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:48.272353Z",
          "iopub.execute_input": "2024-06-04T19:41:48.272771Z",
          "iopub.status.idle": "2024-06-04T19:41:48.299898Z",
          "shell.execute_reply.started": "2024-06-04T19:41:48.272731Z",
          "shell.execute_reply": "2024-06-04T19:41:48.29884Z"
        },
        "trusted": true,
        "id": "UZgiAR4WSI5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"data type of TotalCharges is {df['TotalCharges'].dtype}\")\n",
        "\n",
        "#convert object to float\n",
        "# df['TotalCharges'] = df['TotalCharges'].astype(float) -- commented as I get error - could not convert string to float\n",
        "\n",
        "#There can be blank values which should be converted to NaN first\n",
        "#By using pd.to_numeric with errors='coerce', any non-numeric values (including empty strings) will be replaced with NaN\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "df.describe()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:48.301189Z",
          "iopub.execute_input": "2024-06-04T19:41:48.301522Z",
          "iopub.status.idle": "2024-06-04T19:41:48.336655Z",
          "shell.execute_reply.started": "2024-06-04T19:41:48.301493Z",
          "shell.execute_reply": "2024-06-04T19:41:48.335604Z"
        },
        "trusted": true,
        "id": "yFZOQC9-SI5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the distribution of the numeric predictors**\n",
        "\n",
        "This is cruicial to identify extreame  skewness in any of these features\n",
        "This helps in deciding if we should use the mean or median (in case of skewed data) values to impute missing values.\n",
        "\n",
        "ML algorithims that have normalization assumption e.g. Linear Regression, KNN are sensitive to skewed data hence transformation like logarithmic, square root, cube root, and Box-Cox transformations will improve model performance"
      ],
      "metadata": {
        "id": "7WJxt9C7SI5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "#plot distribution of numeric variables\n",
        "for col in list_numeric_features:\n",
        "    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        sns.histplot(data=df, x=col, kde=True, color='skyblue', edgecolor='black')\n",
        "        plt.title(f'Distribution of {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Density')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:48.33798Z",
          "iopub.execute_input": "2024-06-04T19:41:48.338314Z",
          "iopub.status.idle": "2024-06-04T19:41:49.62073Z",
          "shell.execute_reply.started": "2024-06-04T19:41:48.338285Z",
          "shell.execute_reply": "2024-06-04T19:41:49.619631Z"
        },
        "trusted": true,
        "id": "Clpk5_2uSI5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "- total charges is right skewed. majority of customer have total charges between 18 (minimum value as seen in summary statistics results earlier) to 2000\n",
        "\n",
        "\n",
        "- distribution of tenure represents a bimodal curve as it has 2 peaks at the extremes\n"
      ],
      "metadata": {
        "id": "Oj2p_diASI5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check for outliears**"
      ],
      "metadata": {
        "id": "KtIjAtzRSI5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check for outliers\n",
        "#outlier detection\n",
        "for col in list_numeric_features:\n",
        "    plt.figure(figsize=(4,4))\n",
        "    sns.boxplot(data=df[col])\n",
        "    plt.title(f'Box Plot for {col}')\n",
        "    plt.xlabel('Values')\n",
        "    plt.ylabel(col)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "#OBSERVATION - No outliers in the data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:49.622275Z",
          "iopub.execute_input": "2024-06-04T19:41:49.622656Z",
          "iopub.status.idle": "2024-06-04T19:41:50.313848Z",
          "shell.execute_reply.started": "2024-06-04T19:41:49.622625Z",
          "shell.execute_reply": "2024-06-04T19:41:50.312754Z"
        },
        "trusted": true,
        "id": "LoHZ1usBSI5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore numeric features and their relationship with the outcome - Churn**"
      ],
      "metadata": {
        "id": "ii3zzRseSI5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# compare mean of those who churn vs those who don't\n",
        "for col in list_numeric_features:\n",
        "    plt.figure(figsize=(2,2))\n",
        "    group_mean = df.groupby('Churn')[col].mean()\n",
        "    group_mean.plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Churn')\n",
        "    plt.title(f'Mean {col} by Churn')\n",
        "    plt.show()\n",
        "\n",
        "#Compare the mean values of numeric features for customer who churned vs who did not\n",
        "for col in list_numeric_features:\n",
        "    plt.figure(figsize=(2,2))\n",
        "    sns.boxplot(x='Churn', y=col, data=df)\n",
        "    plt.title(f'Box Plot for {col} Grouped by Churn')\n",
        "    plt.xlabel('Churn')\n",
        "    plt.ylabel(col)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:50.315694Z",
          "iopub.execute_input": "2024-06-04T19:41:50.316058Z",
          "iopub.status.idle": "2024-06-04T19:41:51.634073Z",
          "shell.execute_reply.started": "2024-06-04T19:41:50.316028Z",
          "shell.execute_reply": "2024-06-04T19:41:51.632849Z"
        },
        "trusted": true,
        "id": "FKRnT4PCSI5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "  - Average tenure of those who churn is very low compared to that of customers who stayed\n",
        "    - This suggestss that churn likely happens in  early stage of customer journey\n",
        "     - Median tenure of those who churn is only 10 months compared to median tenure of 40 months of those who stayed\n",
        "  - Monthly charges of those who churn are higher than that of customers who stayed\n",
        "  - Total charges are higher for those who stayed\n"
      ],
      "metadata": {
        "id": "21c7VCc0SI5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare the Kernal Density plots of  these numeric variables for customers who chrun vs who don't.**\n",
        "\n",
        "KDE plots will help us evaluate where the data is most concentrated in each category and help in\n",
        "visually identifying if there is good enough variability in distribution of these numeric features making them a good predictor.\n"
      ],
      "metadata": {
        "id": "HiIMBXaRSI5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in list_numeric_features:\n",
        "    plt.figure(figsize=(3, 2))\n",
        "\n",
        "    # KDE plot of loans that were repaid on time\n",
        "    sns.kdeplot(df.loc[df['Churn'] == 'Yes', col], label='Churn == Yes')\n",
        "    sns.kdeplot(df.loc[df['Churn'] == 'No', col], label='Churn == No')\n",
        "\n",
        "    # Labeling of plot\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Density')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "\n",
        "    # Add legend\n",
        "    plt.legend()\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:51.635406Z",
          "iopub.execute_input": "2024-06-04T19:41:51.63575Z",
          "iopub.status.idle": "2024-06-04T19:41:52.933852Z",
          "shell.execute_reply.started": "2024-06-04T19:41:51.635718Z",
          "shell.execute_reply": "2024-06-04T19:41:52.932691Z"
        },
        "trusted": true,
        "id": "7cenJW40SI5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "- The KDE plot of churned vs not churned customers shows variability in distribution for all three numeric variables\n",
        "- This means these variables can be good indicators of likelihood of churn\n",
        "- We need to now understand the intercorralation among these predictors"
      ],
      "metadata": {
        "id": "EgvaaKNmSI5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.jointplot(data=df, x='tenure', y='MonthlyCharges', palette='Set2', hue='Churn')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:52.935412Z",
          "iopub.execute_input": "2024-06-04T19:41:52.935723Z",
          "iopub.status.idle": "2024-06-04T19:41:54.195894Z",
          "shell.execute_reply.started": "2024-06-04T19:41:52.935696Z",
          "shell.execute_reply": "2024-06-04T19:41:54.194641Z"
        },
        "trusted": true,
        "id": "BnJA6KdgSI5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation matrix to check for itercorrelation/ interactions among numeric predictors**\n",
        "\n",
        "Total Charges is highly correlated with Tenure and Monthly Charges.\n",
        "Essentially total charges = Tenure * Monthly Charges\n",
        "So to avoid multi-collinarity issues we will exclude Total Charges for building the predictive model"
      ],
      "metadata": {
        "id": "U3Eqw_P8SI5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate correlation among independent numericx variables\n",
        "print('Variable correlation')\n",
        "plt.figure(figsize=(2, 2))\n",
        "\n",
        "%matplotlib inline\n",
        "corr = df[list_numeric_features].corr()\n",
        "\n",
        "f, ax = plt.subplots(figsize=(10,12))\n",
        "\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "_ = sns.heatmap(corr, cmap=\"YlGn\", square=True, ax=ax, annot=True, linewidth=0.1)\n",
        "\n",
        "plt.title(\"Pearson correlation of Features\", y=1.05, size=15)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:54.197305Z",
          "iopub.execute_input": "2024-06-04T19:41:54.197639Z",
          "iopub.status.idle": "2024-06-04T19:41:54.633079Z",
          "shell.execute_reply.started": "2024-06-04T19:41:54.197608Z",
          "shell.execute_reply": "2024-06-04T19:41:54.631874Z"
        },
        "trusted": true,
        "id": "k767WI1fSI5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to do filter based feature selection by testing the relation between churn (category) and the numeric variables\n",
        "- ANOVA can be used to check relation between category and numeric variables - if will tell us if the mean of the numeric variables\n",
        "differs significantly among groups. But the assumption is that the numeric variable follows normal distribution.\n",
        "This is not the case here so cannot use ANOVA\n",
        "\n",
        "- Non parametric alternative -  Kruskal–Wallis test - used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups.\n",
        "\n",
        "- one way is to log transform monthly charges to get normal distribution and then run anova\n",
        "\n",
        "to check difference in distributin mannwhitneyu test also known as  Wilcoxon rank-sum test"
      ],
      "metadata": {
        "id": "S9s3I3IxSI5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kruskal–Wallis test\n",
        "from scipy.stats import kruskal\n",
        "sig_num_vars = []\n",
        "for col in list_numeric_features:\n",
        "    groups = df.groupby('Churn')[col].apply(list).values\n",
        "    stat, p_value = kruskal(*groups)\n",
        "    print(f'Kruskal-Wallis H test statistic: {stat}, p-value: {p_value}')\n",
        "    if p_value <= 0.05:\n",
        "     sig_num_vars.append(col)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:54.640015Z",
          "iopub.execute_input": "2024-06-04T19:41:54.640397Z",
          "iopub.status.idle": "2024-06-04T19:41:54.673475Z",
          "shell.execute_reply.started": "2024-06-04T19:41:54.640365Z",
          "shell.execute_reply": "2024-06-04T19:41:54.672057Z"
        },
        "trusted": true,
        "id": "oSFZNok0SI5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sig_num_vars"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:54.674772Z",
          "iopub.execute_input": "2024-06-04T19:41:54.67516Z",
          "iopub.status.idle": "2024-06-04T19:41:54.682664Z",
          "shell.execute_reply.started": "2024-06-04T19:41:54.675126Z",
          "shell.execute_reply": "2024-06-04T19:41:54.68138Z"
        },
        "trusted": true,
        "id": "6ufJm0zPSI5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring Categorical Features**\n",
        "\n",
        "- We will look at how the chrun rate differs across categories\n",
        "- We will also run Chi square test of independence to identify categorical variables that have signifiant relation with the Chrun category we want to predict.\n"
      ],
      "metadata": {
        "id": "xx8CeF4XSI5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:54.683971Z",
          "iopub.execute_input": "2024-06-04T19:41:54.684772Z",
          "iopub.status.idle": "2024-06-04T19:41:54.712906Z",
          "shell.execute_reply.started": "2024-06-04T19:41:54.684727Z",
          "shell.execute_reply": "2024-06-04T19:41:54.711655Z"
        },
        "trusted": true,
        "id": "E70l_y-DSI5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['SeniorCitizen'] = df['SeniorCitizen'].astype('str')\n",
        "print(f\"data type of TotalCharges is {df['SeniorCitizen'].dtype}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:54.714825Z",
          "iopub.execute_input": "2024-06-04T19:41:54.715642Z",
          "iopub.status.idle": "2024-06-04T19:41:54.729662Z",
          "shell.execute_reply.started": "2024-06-04T19:41:54.715598Z",
          "shell.execute_reply": "2024-06-04T19:41:54.72848Z"
        },
        "trusted": true,
        "id": "SGnuV5-NSI50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list the categorical features, check how many distinct values each category has\n",
        "cat_vars = df.select_dtypes(include=['O']).columns.tolist()\n",
        "print(f'number categorical features: {len(cat_vars)}')\n",
        "#The 'Churn' response variable is removed for the cat_vars list\n",
        "cat_vars.remove('Churn')\n",
        "print(df[cat_vars].nunique())\n",
        "#remove customer ID from the list\n",
        "cat_vars.remove('customerID')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:54.731281Z",
          "iopub.execute_input": "2024-06-04T19:41:54.731688Z",
          "iopub.status.idle": "2024-06-04T19:41:54.763226Z",
          "shell.execute_reply.started": "2024-06-04T19:41:54.731652Z",
          "shell.execute_reply": "2024-06-04T19:41:54.762113Z"
        },
        "trusted": true,
        "id": "7rX44HuhSI50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#running the chi square test\n",
        "\n",
        "# Loop through each categorical feature and identify those that are significant\n",
        "sig_cat_vars = []\n",
        "for col in cat_vars:\n",
        "\n",
        "    # Create a contingency table\n",
        "    contingency_table = pd.crosstab(df[col], df['Churn'])\n",
        "\n",
        "    # Perform chi-square test\n",
        "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
        "\n",
        "    # print(f\"Chi-square p-value for {col}: {p}\")\n",
        "    if p <= 0.05:\n",
        "     sig_cat_vars.append(col)\n",
        "print(len(sig_cat_vars))\n",
        "print(sig_cat_vars)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:54.765366Z",
          "iopub.execute_input": "2024-06-04T19:41:54.76583Z",
          "iopub.status.idle": "2024-06-04T19:41:54.900642Z",
          "shell.execute_reply.started": "2024-06-04T19:41:54.765768Z",
          "shell.execute_reply": "2024-06-04T19:41:54.899485Z"
        },
        "trusted": true,
        "id": "hkgxGXQZSI50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Of the 16 categorical variables, 13 have significant relation with the Chrun category we want to predict\n",
        "- Let us further look at the churn rate for each of these categories\n"
      ],
      "metadata": {
        "id": "Q2wP-BH9SI50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the risk % by various categorical variables\n",
        "for col in sig_cat_vars:\n",
        "    plt.figure(figsize=(2, 2))\n",
        "    risk_pct = df.groupby(col).apply(lambda x: x['Churn_Target'].sum() * 100/ len(x['Churn_Target']))\n",
        "#     print(f'% of customers at risk by {col}')\n",
        "    print(risk_pct.plot.bar())\n",
        "    # Set title and labels\n",
        "    plt.title('Churn Rate by ' + col)  # Set the title\n",
        "    plt.xlabel(col)  # Set the label for the x-axis\n",
        "    plt.ylabel('Churn Rate')  # Set the label for the y-axis\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:54.90181Z",
          "iopub.execute_input": "2024-06-04T19:41:54.902118Z",
          "iopub.status.idle": "2024-06-04T19:41:58.007915Z",
          "shell.execute_reply.started": "2024-06-04T19:41:54.90209Z",
          "shell.execute_reply": "2024-06-04T19:41:58.006774Z"
        },
        "trusted": true,
        "id": "THwd7bAZSI50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "- The churn rate is high among senior citizens\n",
        "- churn rate is higher among those who don't have partners or dependents\n",
        "- customers will multiple lines or those with fiber optic connection type have higher churn rate\n",
        "- those who don't have online security, online backup, tech support or online device protection have higher churn rate\n",
        "- churn rate is lower among those who do not have streaming TV or Movies\n",
        "- Those with month - to - month contract have higher churn rate\n",
        "- online savy customers who have paperless billing and pay through electronic checks have higher churn rate\n"
      ],
      "metadata": {
        "id": "EkTchfxvSI50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection\n",
        "\n",
        "- Step 1: Split data in Train (80%) and Test (20%) sets. We want to do feature selection based on Training set to avoid data leakage\n",
        "\n",
        "- Step 2: Create data pipeline with steps for feature transformation (One Hot Encoding and Standard Scalar) and RFECV (Recurrsive Feature Elimination CV) using Random Forest Classifier. Since we have imbalanced class we will use F1 score for scoring\n",
        "\n",
        "- Fit this pipeline on training data and get the optimal features. Cross check with the list of features having significant relation with target.\n",
        "  We executed the significance tests and identified significant numeric and categorical features in data exploration phase.\n"
      ],
      "metadata": {
        "id": "OFJXn_ABSI53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:58.009273Z",
          "iopub.execute_input": "2024-06-04T19:41:58.009584Z",
          "iopub.status.idle": "2024-06-04T19:41:58.016309Z",
          "shell.execute_reply.started": "2024-06-04T19:41:58.009555Z",
          "shell.execute_reply": "2024-06-04T19:41:58.015303Z"
        },
        "trusted": true,
        "id": "dBHG_27MSI53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "X = df.drop(['Churn_Target','customerID', 'Churn'], axis=1)\n",
        "y = df['Churn_Target']\n",
        "# X.head()\n",
        "\n",
        "#Split data in Train 80%  and Test 20% groups using stratified split on target .\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)\n",
        "\n",
        "categorical_preprocessor = OneHotEncoder(drop='first', handle_unknown=\"ignore\")\n",
        "numerical_preprocessor = StandardScaler()\n",
        "\n",
        "categorical_vars = X.select_dtypes(include=['O']).columns.tolist()\n",
        "\n",
        "# numeric_vars = X.select_dtypes(np.number).columns.tolist()\n",
        "\n",
        "\n",
        "FS_preprocessor = ColumnTransformer(\n",
        "    [\n",
        "        (\"one-hot-encoder\", categorical_preprocessor, categorical_vars),\n",
        "        (\"standard-scaler\", numerical_preprocessor, sig_num_vars)\n",
        "    ]\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators = 1000, min_samples_leaf =10, random_state = 42)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3)\n",
        "\n",
        "# Recurcive feature elimination with cross validation\n",
        "rfecv = RFECV(\n",
        "    estimator=rf,\n",
        "    step=1,\n",
        "    cv= cv,\n",
        "    min_features_to_select = 1,\n",
        "    scoring='neg_log_loss',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:58.017621Z",
          "iopub.execute_input": "2024-06-04T19:41:58.01795Z",
          "iopub.status.idle": "2024-06-04T19:41:58.048548Z",
          "shell.execute_reply.started": "2024-06-04T19:41:58.017922Z",
          "shell.execute_reply": "2024-06-04T19:41:58.047543Z"
        },
        "trusted": true,
        "id": "7e_ZB775SI54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the random forest feature importance executed on entire training data\n",
        "fs_model_pipeline_rf = Pipeline(steps=[\n",
        "    ('preprocessor', FS_preprocessor),\n",
        "    ('classifier', rf)\n",
        "])\n",
        "\n",
        "\n",
        "fs_model_pipeline_rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:41:58.0499Z",
          "iopub.execute_input": "2024-06-04T19:41:58.050249Z",
          "iopub.status.idle": "2024-06-04T19:42:03.804443Z",
          "shell.execute_reply.started": "2024-06-04T19:41:58.050219Z",
          "shell.execute_reply": "2024-06-04T19:42:03.803348Z"
        },
        "trusted": true,
        "id": "2KAXiZQ9SI54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fs_model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', FS_preprocessor),\n",
        "    ('classifier', rfecv)\n",
        "])\n",
        "\n",
        "\n",
        "fs_model_pipeline.fit(X_train, y_train)\n",
        "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:42:03.80589Z",
          "iopub.execute_input": "2024-06-04T19:42:03.806326Z",
          "iopub.status.idle": "2024-06-04T19:46:34.582459Z",
          "shell.execute_reply.started": "2024-06-04T19:42:03.806286Z",
          "shell.execute_reply": "2024-06-04T19:46:34.581054Z"
        },
        "trusted": true,
        "id": "pvGN7L1fSI54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessor.fit(X_train)\n",
        "feature_names = fs_model_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
        "\n",
        "best_features = feature_names[rfecv.support_].tolist()\n",
        "print(\"Total Features:\", feature_names.shape)\n",
        "\n",
        "print(\"Best Features: %s\" % len(best_features))\n",
        "\n",
        "print(\"Best Features List: %s\" % best_features)\n",
        "\n",
        "\n",
        "# -- in RFECV first random forest algo is applied on entire data. In this case training data. The feature importance is calculated\n",
        "# -- then each least imp feature is removed one at a time. Random forest is executed on remaining features in cross validation - the value of evaluation metric is recorded\n",
        "# - this goes on until no features are left to remove\n",
        "# the set of features which gave optimal result for evaluation metric are selected\n",
        "# - can we do a nested cross val for more robut feature selection where in feature selection is done on each of the fold and RFECV is applied to that fold.\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:46:34.584103Z",
          "iopub.execute_input": "2024-06-04T19:46:34.584433Z",
          "iopub.status.idle": "2024-06-04T19:46:34.593288Z",
          "shell.execute_reply.started": "2024-06-04T19:46:34.584399Z",
          "shell.execute_reply": "2024-06-04T19:46:34.592059Z"
        },
        "trusted": true,
        "id": "ilUF2o-ASI54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('gender').nunique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:46:34.594965Z",
          "iopub.execute_input": "2024-06-04T19:46:34.595397Z",
          "iopub.status.idle": "2024-06-04T19:46:34.651956Z",
          "shell.execute_reply.started": "2024-06-04T19:46:34.595358Z",
          "shell.execute_reply": "2024-06-04T19:46:34.650654Z"
        },
        "trusted": true,
        "id": "bcjQa7_bSI55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model Build and Evaluate (Run ML Experiment)\n",
        "\n",
        "- Among all features those that are best features as per RFECV are used further to develop ML Models\n",
        "- Create function to train and evaluate model. Models are training only on the training data using 4 fold cross validation\n",
        "- define list of models to experiment and loop through the list, calling above function in each iteration\n",
        "- store results of all model experiments in data frame\n",
        "\n",
        "Note - we will run logistic regression, Random Forest, Random Forest with class weight balanced, XGBoost and NN.\n",
        "Models will be evaluated for 2 different prediction goals\n",
        "\n",
        "    - Predict the class where output is whether someone will churn or not. For this models are scored using ROC_AUC and F1 Score\n",
        "    \n",
        "    - Predict the liklihood of churn where the output is probability of churn (0 to 1). For this models are scored using Log loss\n",
        "    \n",
        "    **The optimal model differs based on whether we want to predict class or probability of being in a class\n"
      ],
      "metadata": {
        "id": "u_bK1v5rSI55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to run ML experiments**"
      ],
      "metadata": {
        "id": "J7yXeaI8SI55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ml_model_run(X_train, X_test, Y_train, Y_test, ml_model, ml_model_desc, cv, score):\n",
        "\n",
        "    cv_scores = cross_validate(ml_model, X_train, Y_train, cv=cv, scoring = score,  return_train_score=True)\n",
        "\n",
        "    mean_scores = {}\n",
        "    for s in score:\n",
        "        mean_scores[f'mean_train_{s}'] = np.mean(cv_scores[f'train_{s}'])\n",
        "        mean_scores[f'mean_test_{s}'] = np.mean(cv_scores[f'test_{s}'])\n",
        "\n",
        "\n",
        "    result = [ml_model_desc,ml_model.get_params(),score,mean_scores]\n",
        "\n",
        "\n",
        "    return result\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:46:34.653231Z",
          "iopub.execute_input": "2024-06-04T19:46:34.653546Z",
          "iopub.status.idle": "2024-06-04T19:46:34.661373Z",
          "shell.execute_reply.started": "2024-06-04T19:46:34.653519Z",
          "shell.execute_reply": "2024-06-04T19:46:34.660168Z"
        },
        "trusted": true,
        "id": "t_tyPe_bSI55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline model with all features**"
      ],
      "metadata": {
        "id": "pXiOdYiJSI55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Further use features selected from RFE CV for model building\n",
        "FS_preprocessor.fit(X_train)\n",
        "\n",
        "X_train_ohe = pd.DataFrame(FS_preprocessor.transform(X_train),columns=FS_preprocessor.get_feature_names_out())\n",
        "X_test_ohe = pd.DataFrame(FS_preprocessor.transform(X_test),columns=FS_preprocessor.get_feature_names_out())\n",
        "\n",
        "X_train_fs = X_train_ohe[best_features]\n",
        "X_test_fs = X_test_ohe[best_features]\n",
        "\n",
        "print(X_train_ohe.shape)\n",
        "print(X_train_fs.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:46:34.663053Z",
          "iopub.execute_input": "2024-06-04T19:46:34.663488Z",
          "iopub.status.idle": "2024-06-04T19:46:34.809242Z",
          "shell.execute_reply.started": "2024-06-04T19:46:34.663446Z",
          "shell.execute_reply": "2024-06-04T19:46:34.808193Z"
        },
        "trusted": true,
        "id": "zeVpHsO6SI55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators = 100, min_samples_leaf =4, random_state = 42)\n",
        "rf_balanced = RandomForestClassifier(n_estimators = 100, min_samples_leaf =4, random_state = 42, class_weight='balanced')\n",
        "xgb_classifier = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "log_reg = LogisticRegression()\n",
        "nn = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)\n",
        "clf_dummy = DummyClassifier(random_state=42, strategy = 'most_frequent')\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "score = ['roc_auc', 'recall', 'f1', 'neg_log_loss']\n",
        "# metric = m.roc_auc_score\n",
        "model_list = {'Baseline Predicting most frequest class':  clf_dummy,\n",
        "    'Random Forest': rf,\n",
        "    'Random Forest - Class wt balanced': rf_balanced,\n",
        "              'XGBoost': xgb_classifier,\n",
        "              'Logistic' : log_reg,\n",
        "              'nn' : nn\n",
        "             }\n",
        "ml_expriments_df = pd.DataFrame(columns = ['model_name', 'config', 'metric', 'cv_score'])\n",
        "\n",
        "#baseline model training on 29 features\n",
        "\n",
        "# baseline probabilities -- predicting the most frequent class that is 0 in this case\n",
        "probabilities = [0 for _ in range(len(y_test))]\n",
        "avg_brier = brier_score_loss(y_test, probabilities)\n",
        "avg_log_loss = log_loss(y_test, probabilities)\n",
        "print('Baseline: Brier Score=%.4f' % (avg_brier))\n",
        "print('Baseline: log_loss=%.4f' % (avg_log_loss))\n",
        "print('Baseline: F1=%.4f' % (f1_score(y_test, probabilities)))\n",
        "\n",
        "# probabilities = [0.265 for _ in range(len(y_test))]\n",
        "# avg_brier = brier_score_loss(y_test, probabilities)\n",
        "# avg_log_loss = log_loss(y_test, probabilities)\n",
        "# print('Baseline: Brier Score=%.4f' % (avg_brier))\n",
        "# print('Baseline: log_loss=%.4f' % (avg_log_loss))\n",
        "# # print('Baseline: F1=%.4f' % (f1_score(y_test, probabilities)))\n",
        "\n",
        "# result = ml_model_run(X_train_ohe, X_test_ohe, y_train, y_test, log_reg, 'Baseline Logistic all features', cv, score)\n",
        "# ml_expriments_df.loc[len(ml_expriments_df)] = result\n",
        "\n",
        "#ML models on selected 13 features\n",
        "\n",
        "\n",
        "\n",
        "for k,v in model_list.items():\n",
        "    result = ml_model_run(X_train_fs, X_test_fs, y_train, y_test, v, k, cv, score)\n",
        "    ml_expriments_df.loc[len(ml_expriments_df)] = result\n",
        "\n",
        "ml_expriments_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:46:34.810488Z",
          "iopub.execute_input": "2024-06-04T19:46:34.810857Z",
          "iopub.status.idle": "2024-06-04T19:46:45.380476Z",
          "shell.execute_reply.started": "2024-06-04T19:46:34.810821Z",
          "shell.execute_reply": "2024-06-04T19:46:45.378754Z"
        },
        "trusted": true,
        "id": "mQQI7C6KSI55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fixed = ml_expriments_df.join(pd.json_normalize(ml_expriments_df['cv_score'])).drop('cv_score', axis='columns')\n",
        "df_fixed"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:46:45.383392Z",
          "iopub.execute_input": "2024-06-04T19:46:45.384551Z",
          "iopub.status.idle": "2024-06-04T19:46:45.438345Z",
          "shell.execute_reply.started": "2024-06-04T19:46:45.384469Z",
          "shell.execute_reply": "2024-06-04T19:46:45.435015Z"
        },
        "trusted": true,
        "id": "pzAkQz9_SI55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation\n",
        "\n",
        "- Predicting class\n",
        "Based on the F1 score of the test fold of Cross Valudation, Random Forest with Class weight Balanced parameter is the best performaing model. Since the score for training folds is higher here it means there is an overfitting problem. To address this issue we can explore hyperparameter runing like decreasing tree depth, increasing n_estimators (number of trees), evaluate the feature importance of this model and test model outcome on selecting top n features using CV, since this is imbalanced class we can increase data size by creating 50-50 distribution of each class or suing SMOTE.\n",
        "\n",
        "- Predicting Class probability\n",
        "Based on the neg_log_loss of the test fold of Cross Valudation, Neural Network with 1 hidden layer of 10 neurons performs best. In this notebook, we aim to predict class probability so as next steps we will train this model on entire training data and evaluate the log loss, we will also examin the model calibration (compare the distribution of predict probability vs actual class probability) and develop a calibratated model to improve prediction accuracy.\n"
      ],
      "metadata": {
        "id": "3MBIoqiNSI56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "otpbTfBxSI56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run selected model on entire training data and record predictions and scoring metric on test\n",
        "# Predict probabilities on the training set\n",
        "nn.fit(X_train_fs, y_train)\n",
        "\n",
        "y_train_prob = nn.predict_proba(X_train_fs)[:, 1]\n",
        "\n",
        "\n",
        "print(f'Training Data Log Loss {m.log_loss(y_train, y_train_prob)}')\n",
        "\n",
        "y_test_prob = nn.predict_proba(X_test_fs)[:, 1]\n",
        "\n",
        "print(f'Test Data Log Loss {m.log_loss(y_test, y_test_prob)}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:46:45.440448Z",
          "iopub.execute_input": "2024-06-04T19:46:45.441114Z",
          "iopub.status.idle": "2024-06-04T19:46:47.877606Z",
          "shell.execute_reply.started": "2024-06-04T19:46:45.441063Z",
          "shell.execute_reply": "2024-06-04T19:46:47.876162Z"
        },
        "trusted": true,
        "id": "TIHNmWijSI56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calibrated Model Hyperparameter tuning\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(10,), (32,), (10, 5)],\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'alpha': [0.0001, 0.001],\n",
        "    'max_iter': [1000, 500]\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(nn, param_grid, cv=3, scoring='neg_log_loss', n_jobs=-1)\n",
        "grid_search.fit(X_train_fs, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:46:47.879692Z",
          "iopub.execute_input": "2024-06-04T19:46:47.880409Z",
          "iopub.status.idle": "2024-06-04T19:53:15.815349Z",
          "shell.execute_reply.started": "2024-06-04T19:46:47.880354Z",
          "shell.execute_reply": "2024-06-04T19:53:15.813864Z"
        },
        "trusted": true,
        "id": "6fqnoFwjSI56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_tuned = MLPClassifier(hidden_layer_sizes=(10,5), max_iter=1000, activation = 'tanh', alpha = 0.001, learning_rate = 'constant', solver='adam' , random_state=42)\n",
        "\n",
        "nn_tuned.fit(X_train_fs, y_train)\n",
        "\n",
        "# Predict probabilities on the training set\n",
        "y_train_prob = nn_tuned.predict_proba(X_train_fs)[:, 1]\n",
        "\n",
        "\n",
        "print(f'Training Data Log Loss {m.log_loss(y_train, y_train_prob)}')\n",
        "\n",
        "y_test_prob = nn_tuned.predict_proba(X_test_fs)[:, 1]\n",
        "\n",
        "print(f'Test Data Log Loss {m.log_loss(y_test, y_test_prob)}')\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:53:15.817062Z",
          "iopub.execute_input": "2024-06-04T19:53:15.817458Z",
          "iopub.status.idle": "2024-06-04T19:53:20.081857Z",
          "shell.execute_reply.started": "2024-06-04T19:53:15.817424Z",
          "shell.execute_reply": "2024-06-04T19:53:20.079713Z"
        },
        "trusted": true,
        "id": "SQKyDxnMSI56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concept of Calibration\n",
        "- In predicting probability of class we can further assess the accuracy of prediction by plotting calibration curve\n",
        "    - divide data in bins based on predicted probabilities\n",
        "    - for each bin (eg. bin 1 predicticed probability range from 0 to 0.2) get the actual probability/ fraction of positives = number of actual positive labels (found using actual class label)/ total records in the bin\n",
        "    - Calculate predicted probability = mean of the predicted probabilities in that bin for that class\n",
        "    - plot the actual probabilities vs predicted probabilities. Ideally perfect prediction would mean for each bin actual probability = predicted probability denoted by the diagonal line for perfect calibration\n",
        "    \n",
        " Why we need calibration\n",
        "\n"
      ],
      "metadata": {
        "id": "tWBgF4biSI56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the calibration curve\n",
        "prob_true, prob_pred = calibration_curve(y_test, y_test_prob, n_bins=10)\n",
        "# Plot the calibration curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title('Calibration Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:54:25.218216Z",
          "iopub.execute_input": "2024-06-04T19:54:25.218835Z",
          "iopub.status.idle": "2024-06-04T19:54:25.577173Z",
          "shell.execute_reply.started": "2024-06-04T19:54:25.218752Z",
          "shell.execute_reply": "2024-06-04T19:54:25.575833Z"
        },
        "trusted": true,
        "id": "cub_Y04ZSI56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y_test_prob, bins=10)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:54:38.027374Z",
          "iopub.execute_input": "2024-06-04T19:54:38.027907Z",
          "iopub.status.idle": "2024-06-04T19:54:38.351872Z",
          "shell.execute_reply.started": "2024-06-04T19:54:38.02786Z",
          "shell.execute_reply": "2024-06-04T19:54:38.350552Z"
        },
        "trusted": true,
        "id": "BxadrSFHSI56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# using grid search CV to identify optimal parameter serring for the calibrated classifier\n",
        "\n",
        "# wrap the model\n",
        "calibrated = CalibratedClassifierCV(nn_tuned)\n",
        "\n",
        "\n",
        "# define grid\n",
        "param_grid = dict(cv=[2,3], method=['sigmoid','isotonic'])\n",
        "\n",
        "# define grid search\n",
        "grid = GridSearchCV(estimator=calibrated, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='neg_log_loss')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X_train_fs, y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:54:50.550979Z",
          "iopub.execute_input": "2024-06-04T19:54:50.551474Z",
          "iopub.status.idle": "2024-06-04T19:55:46.465869Z",
          "shell.execute_reply.started": "2024-06-04T19:54:50.551438Z",
          "shell.execute_reply": "2024-06-04T19:55:46.4638Z"
        },
        "trusted": true,
        "id": "uu3Lzm-_SI57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# report the best configuration\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "# report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:57:38.203124Z",
          "iopub.execute_input": "2024-06-04T19:57:38.203633Z",
          "iopub.status.idle": "2024-06-04T19:57:38.213295Z",
          "shell.execute_reply.started": "2024-06-04T19:57:38.203595Z",
          "shell.execute_reply": "2024-06-04T19:57:38.211331Z"
        },
        "trusted": true,
        "id": "XlIBN_kCSI57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check calibration curve now\n",
        "best_calibrated = CalibratedClassifierCV(nn_tuned, method='isotonic', cv=3)\n",
        "# Train a Random Forest classifier\n",
        "\n",
        "best_calibrated.fit(X_train_fs, y_train)\n",
        "\n",
        "# Predict probabilities on the training set\n",
        "y_train_prob = best_calibrated.predict_proba(X_train_fs)[:, 1]\n",
        "\n",
        "\n",
        "print(f'Training Data Log Loss {m.log_loss(y_train, y_train_prob)}')\n",
        "\n",
        "y_test_prob = best_calibrated.predict_proba(X_test_fs)[:, 1]\n",
        "\n",
        "print(f'Test Data Log Loss {m.log_loss(y_test, y_test_prob)}')\n",
        "\n",
        "# Calculate the calibration curve\n",
        "prob_true, prob_pred = calibration_curve(y_test, y_test_prob, n_bins=10)\n",
        "# Plot the calibration curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title('Calibration Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#plot probability distribution of actual vs predicted probability\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T20:01:07.028512Z",
          "iopub.execute_input": "2024-06-04T20:01:07.029099Z",
          "iopub.status.idle": "2024-06-04T20:01:18.438623Z",
          "shell.execute_reply.started": "2024-06-04T20:01:07.029058Z",
          "shell.execute_reply": "2024-06-04T20:01:18.436951Z"
        },
        "trusted": true,
        "id": "pnBmirpmSI57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot distribution of predicted probabilities\n",
        "plt.hist(y_test_prob, bins=10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T20:01:28.912499Z",
          "iopub.execute_input": "2024-06-04T20:01:28.91373Z",
          "iopub.status.idle": "2024-06-04T20:01:29.217647Z",
          "shell.execute_reply.started": "2024-06-04T20:01:28.913644Z",
          "shell.execute_reply": "2024-06-04T20:01:29.216059Z"
        },
        "trusted": true,
        "id": "Usg4-ZSUSI57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calibration did recude the log loss which is indicating model improvement in predicting probability"
      ],
      "metadata": {
        "id": "lHP6YlWJSI57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #conducting SMOTE to improve model performance\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# smote = SMOTE(random_state=42)\n",
        "# X_train_smote, y_train_smote = smote.fit_resample(X_train_fs, y_train)\n",
        "# # wrap the model\n",
        "# calibrated = CalibratedClassifierCV(nn)\n",
        "\n",
        "\n",
        "# # define grid\n",
        "# param_grid = dict(cv=[2,3,4], method=['sigmoid','isotonic'])\n",
        "\n",
        "# # define grid search\n",
        "# grid = GridSearchCV(estimator=calibrated, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='neg_log_loss')\n",
        "# # execute the grid search\n",
        "# grid_result = grid.fit(X_train_smote, y_train_smote)\n",
        "# # report the best configuration\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "# # report all configurations\n",
        "# means = grid_result.cv_results_['mean_test_score']\n",
        "# stds = grid_result.cv_results_['std_test_score']\n",
        "# params = grid_result.cv_results_['params']\n",
        "# for mean, stdev, param in zip(means, stds, params):\n",
        "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:54:24.768575Z",
          "iopub.execute_input": "2024-06-04T19:54:24.769098Z",
          "iopub.status.idle": "2024-06-04T19:54:24.776913Z",
          "shell.execute_reply.started": "2024-06-04T19:54:24.769058Z",
          "shell.execute_reply": "2024-06-04T19:54:24.775412Z"
        },
        "trusted": true,
        "id": "Hg1djKKCSI57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OL1V8HTKSI57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#without using RFE do cross val on training - each fold pict top 40% important features\n",
        "#then combine results by creating final list of features (include features apprearing at least in one fold)\n",
        "# custome code"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:54:24.786235Z",
          "iopub.execute_input": "2024-06-04T19:54:24.786706Z",
          "iopub.status.idle": "2024-06-04T19:54:24.795991Z",
          "shell.execute_reply.started": "2024-06-04T19:54:24.786655Z",
          "shell.execute_reply": "2024-06-04T19:54:24.794343Z"
        },
        "trusted": true,
        "id": "sIXpBWMxSI57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # -- another approach\n",
        "# # -- create pipeline one hot encode -  rf estimator\n",
        "# # -- start cross val 4 fold loop - in each fold run pipeline get selected features\n",
        "\n",
        "# fs_model_pipeline_2 = Pipeline(steps=[\n",
        "#     ('preprocessor', FS_preprocessor),\n",
        "#     ('classifier', rfecv)\n",
        "# ])\n",
        "\n",
        "# scv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# # Set to hold selected features from each fold\n",
        "# # -- set because I only want features found in rfecv of each fold (intersection features)\n",
        "# # selected_features = set()\n",
        "# feature_names = fs_model_pipeline_2.named_steps['preprocessor'].get_feature_names_out()\n",
        "# feature_counter = {feature: 0 for feature in feature_names}\n",
        "# # feature_counter\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:54:24.798149Z",
          "iopub.execute_input": "2024-06-04T19:54:24.798718Z",
          "iopub.status.idle": "2024-06-04T19:54:24.809767Z",
          "shell.execute_reply.started": "2024-06-04T19:54:24.798643Z",
          "shell.execute_reply": "2024-06-04T19:54:24.808323Z"
        },
        "trusted": true,
        "id": "DY4mQW3ISI57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cross-validation loop\n",
        "# for train_index, val_index in scv.split(X_train, y_train):\n",
        "# #     print(train_index[:10])\n",
        "#     X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "#     y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "#     fs_model_pipeline_2.fit(X_fold_train, y_fold_train)\n",
        "#     print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
        "\n",
        "#     feature_names = fs_model_pipeline_2.named_steps['preprocessor'].get_feature_names_out()\n",
        "#     print(\"Feature names:\", feature_names.shape)\n",
        "\n",
        "#     print(\"Best features: %s\" % feature_names[rfecv.support_].tolist())\n",
        "\n",
        "#     best_features = feature_names[rfecv.support_].tolist()\n",
        "#     # Add these features to the selected_features set\n",
        "\n",
        "#     # Update the feature counter dictionary\n",
        "#     for feature in best_features:\n",
        "#         feature_counter[feature] += 1\n",
        "\n",
        "# # Print the final list of selected features\n",
        "# print(\"Selected features:\", selected_features)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T19:54:24.811708Z",
          "iopub.execute_input": "2024-06-04T19:54:24.812325Z",
          "iopub.status.idle": "2024-06-04T19:54:24.824671Z",
          "shell.execute_reply.started": "2024-06-04T19:54:24.812272Z",
          "shell.execute_reply": "2024-06-04T19:54:24.823272Z"
        },
        "trusted": true,
        "id": "QtEfkxfaSI58"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}